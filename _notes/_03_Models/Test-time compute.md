![](attachments/187e55991a767f13fc000a4b8dd60046_MD5.jpeg)

![](attachments/7bc4675d5e4486a92762b64cfaa244b8_MD5.jpeg)
![](attachments/79b1db41eb2ead02fef9903dbf2873e0_MD5.jpeg)
![](attachments/0446f194cd02dd4d0ec78a26a83a2a5d_MD5.jpeg)

![](attachments/3bf80f68b1b73c6d39c20c6ea7e266fb_MD5.jpeg)

![](attachments/b2a3d1b2c33cb4ce865e26d0f508b172_MD5.jpeg)

![](attachments/4bcfc992405c81461132f3dbfc9142e4_MD5.jpeg)

![](attachments/eb820a5e770d6f7d00074d0a1e9dc2c8_MD5.jpeg)

https://github.com/hijkzzz/Awesome-LLM-Strawberry


# Let's verify step by step

We have shown that process supervision can be used to train much more reliable reward models than outcome supervision in the domain of mathematical reasoning.

https://github.com/openai/prm800k

