
The Llama 3 Herd of models natively supports multilinguality, coding, reasoning, and tool usage.

Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens.

Trained on 15T multilingual tokens

Pretrained using $3.8 X 10^{25}$ FLOPs

**Post training**: 
- supervised finetuning (SFT), 
- rejection sampling (RS), and 
- direct preference optimization (DPO)

**Capabilities:**
- Answer questions in atleast 8 languages
- write high-quality code
- solve complex reasoning problmes
- use tools out-of-box

Multio
